{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3543e457",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "- Initially, I had multiple datasets containing lists of only ad servers and non-ad servers. I combined them all to create a dataset 'all.csv'.\n",
    "- Since all.csv had multiple overlapping entries, I deleted the duplicates and saved it as another file 'all-without-duplicates'\n",
    "```\n",
    "df = pd.read_csv(\"../lists/all.csv\",converters={'domain': convert_dtype,'class': convert_dtype}) \n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../lists/all-without-duplicates.csv')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec64c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcee9bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazonaws.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>netflix.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474708</th>\n",
       "      <td>slview.psne.jp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474709</th>\n",
       "      <td>x.vipergirls.to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474710</th>\n",
       "      <td>x0r.urlgalleries.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474711</th>\n",
       "      <td>yotta.scrolller.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474712</th>\n",
       "      <td>ytre9jk.txxx.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1474713 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          url class\n",
       "0                  google.com     0\n",
       "1                 youtube.com     0\n",
       "2                facebook.com     0\n",
       "3               amazonaws.com     0\n",
       "4                 netflix.com     0\n",
       "...                       ...   ...\n",
       "1474708        slview.psne.jp     1\n",
       "1474709       x.vipergirls.to     1\n",
       "1474710  x0r.urlgalleries.net     1\n",
       "1474711   yotta.scrolller.com     1\n",
       "1474712      ytre9jk.txxx.com     1\n",
       "\n",
       "[1474713 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert dtypes for fixing Dtypewarning\n",
    "# https://www.roelpeters.be/solved-dtypewarning-columns-have-mixed-types-specify-dtype-option-on-import-or-set-low-memory-in-pandas/\n",
    "def convert_dtype(x):\n",
    "    if not x:\n",
    "        return ''\n",
    "    try:\n",
    "        return str(x)   \n",
    "    except:        \n",
    "        return ''\n",
    "\n",
    "df = pd.read_csv(\"../lists/all-without-duplicates.csv\",converters={'domain': convert_dtype,'class': convert_dtype}) # Dataset is now stored in a Pandas Dataframe\n",
    "#df = pd.read_csv(\"../lists/all.csv\",converters={'domain': convert_dtype,'class': convert_dtype})\n",
    "#df = df.drop_duplicates()\n",
    "#df['class'] = df['class'].map({'1': '0', '0': '1'}) # issue w original dataset where non ads were marked as 1 and ads as 0. This reverses it.\n",
    "#df.to_csv('../lists/all-without-duplicates.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1fe32",
   "metadata": {},
   "source": [
    "# Preprocessing and feature extraction\n",
    "This block of code is used for preprocessing the dataset, removing unwanted patterns, and extracting meaningful features from the dataset. Here, the features extracted are has_ad(does it contain the word 'ad'), is_subdomain(does it contain the subdomain 'www'),num_dots(number of dots in the url, excluding subdomain if any),num_hyphens(number of hyphens), num_digits(number of digits in the URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1368e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expressions for pattern matching\n",
    "ad_pattern = r'\\b(ad|ads)\\b'\n",
    "subdomain_pattern = r'^www\\.'\n",
    "dot_pattern = r'.'\n",
    "hyphen_pattern = r'-'\n",
    "digit_pattern = r'\\d'\n",
    "\n",
    "# Define the batch size and the input/output file paths\n",
    "batch_size = 10000\n",
    "input_file = '../lists/all-without-duplicates.csv'\n",
    "output_file = '../lists/preprocessed.csv'\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    # Read the CSV file in chunks\n",
    "    for chunk in pd.read_csv(f_in, chunksize=batch_size):\n",
    "        # Preprocess the URLs in the current chunk\n",
    "        #for url in chunk['url']:\n",
    "        for index, row in chunk.iterrows():\n",
    "            url = row['url']\n",
    "            is_ad = row['class']\n",
    "            has_ad = int(bool(re.search(ad_pattern, url)))\n",
    "            is_subdomain = int(bool(re.search(subdomain_pattern, url)))\n",
    "            num_dots = url.count(dot_pattern) #- is_subdomain\n",
    "            if (is_subdomain == 1):\n",
    "                num_dots = num_dots - 1;\n",
    "            num_hyphens = url.count(hyphen_pattern)\n",
    "            num_digits = len(re.findall(digit_pattern, url))\n",
    "\n",
    "            # Write the preprocessed features to the output file\n",
    "            f_out.write(f'{url},{has_ad},{is_subdomain},{num_dots},{num_hyphens},{num_digits},{is_ad}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21cd885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (1,2,3,4,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>has_ad</th>\n",
       "      <th>is_subdomain</th>\n",
       "      <th>num_dots</th>\n",
       "      <th>num_hyphens</th>\n",
       "      <th>num_digits</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>url</td>\n",
       "      <td>has_ad</td>\n",
       "      <td>is_subdomain</td>\n",
       "      <td>num_dots</td>\n",
       "      <td>num_hyphens</td>\n",
       "      <td>num_digits</td>\n",
       "      <td>is_ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazonaws.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474709</th>\n",
       "      <td>slview.psne.jp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474710</th>\n",
       "      <td>x.vipergirls.to</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474711</th>\n",
       "      <td>x0r.urlgalleries.net</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474712</th>\n",
       "      <td>yotta.scrolller.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474713</th>\n",
       "      <td>ytre9jk.txxx.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1474714 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          url  has_ad  is_subdomain  num_dots  num_hyphens  \\\n",
       "0                         url  has_ad  is_subdomain  num_dots  num_hyphens   \n",
       "1                  google.com       0             0         1            0   \n",
       "2                 youtube.com       0             0         1            0   \n",
       "3                facebook.com       0             0         1            0   \n",
       "4               amazonaws.com       0             0         1            0   \n",
       "...                       ...     ...           ...       ...          ...   \n",
       "1474709        slview.psne.jp       0             0         2            0   \n",
       "1474710       x.vipergirls.to       0             0         2            0   \n",
       "1474711  x0r.urlgalleries.net       0             0         2            0   \n",
       "1474712   yotta.scrolller.com       0             0         2            0   \n",
       "1474713      ytre9jk.txxx.com       0             0         2            0   \n",
       "\n",
       "         num_digits  class  \n",
       "0        num_digits  is_ad  \n",
       "1                 0      0  \n",
       "2                 0      0  \n",
       "3                 0      0  \n",
       "4                 0      0  \n",
       "...             ...    ...  \n",
       "1474709           0      1  \n",
       "1474710           0      1  \n",
       "1474711           1      1  \n",
       "1474712           0      1  \n",
       "1474713           1      1  \n",
       "\n",
       "[1474714 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the batch size, the number of epochs, and the input file path\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "input_file = '../lists/preprocessed.csv'\n",
    "\n",
    "# Load the preprocessed features and the class label into a Pandas DataFrame\n",
    "data = pd.read_csv(input_file, header=None, names=['url', 'has_ad', 'is_subdomain', 'num_dots', 'num_hyphens', 'num_digits', 'class'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f7a9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-daf7746fa2ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Evaluate the model on the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the batch size, the number of epochs, and the input file path\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "input_file = '../lists/preprocessed.csv'\n",
    "\n",
    "# Load the preprocessed features and the class label into a Pandas DataFrame\n",
    "#data = pd.read_csv(input_file, header=None, dtype={'url':str, 'has_ad':int, 'is_subdomain':int, 'num_dots':int, 'num_hyphens':int, 'num_digits':int, 'is_ad':int}, names=['url', 'has_ad', 'is_subdomain', 'num_dots', 'num_hyphens', 'num_digits', 'is_ad'])\n",
    "data = pd.read_csv(input_file, header=None, names=['url', 'has_ad', 'is_subdomain', 'num_dots', 'num_hyphens', 'num_digits', 'class'])\n",
    "data = data.dropna()\n",
    "data = data.replace([np.inf, -np.inf, np.nan], 0)\n",
    "data = data.astype({'has_ad': 'float32', 'is_subdomain': 'float32', 'num_dots': 'float32', 'num_hyphens': 'float32', 'num_digits': 'float32', 'class': 'float32'})\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, stratify=data['class'])\n",
    "\n",
    "\n",
    "\n",
    "# Define the input shape of the neural network\n",
    "input_shape = (train_data.shape[1] - 1,)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data.iloc[:, :-1], train_data.iloc[:, -1], batch_size=batch_size, epochs=epochs, validation_data=(val_data.iloc[:, :-1], val_data.iloc[:, -1]), callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, acc = model.evaluate(val_data.iloc[:, :-1], val_data.iloc[:, -1], batch_size=batch_size)\n",
    "print('Validation accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff531df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26215/26215 [==============================] - 24s 894us/step - loss: 0.0320 - accuracy: 0.9945 - val_loss: 0.0307 - val_accuracy: 0.9945\n",
      "Epoch 2/100\n",
      "26215/26215 [==============================] - 23s 892us/step - loss: 0.0305 - accuracy: 0.9946 - val_loss: 0.0307 - val_accuracy: 0.9946\n",
      "Epoch 3/100\n",
      "26215/26215 [==============================] - 23s 892us/step - loss: 0.0304 - accuracy: 0.9946 - val_loss: 0.0310 - val_accuracy: 0.9945\n",
      "Epoch 4/100\n",
      "26215/26215 [==============================] - 23s 892us/step - loss: 0.0304 - accuracy: 0.9946 - val_loss: 0.0305 - val_accuracy: 0.9945\n",
      "Epoch 5/100\n",
      "26215/26215 [==============================] - 23s 893us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0308 - val_accuracy: 0.9946\n",
      "Epoch 6/100\n",
      "26215/26215 [==============================] - 23s 889us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0307 - val_accuracy: 0.9944\n",
      "Epoch 7/100\n",
      "26215/26215 [==============================] - 23s 892us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0304 - val_accuracy: 0.9945\n",
      "Epoch 8/100\n",
      "26215/26215 [==============================] - 23s 890us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0310 - val_accuracy: 0.9946\n",
      "Epoch 9/100\n",
      "26215/26215 [==============================] - 24s 900us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0308 - val_accuracy: 0.9945\n",
      "Epoch 10/100\n",
      "26215/26215 [==============================] - 24s 908us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0305 - val_accuracy: 0.9945\n",
      "Epoch 11/100\n",
      "26215/26215 [==============================] - 23s 894us/step - loss: 0.0303 - accuracy: 0.9946 - val_loss: 0.0304 - val_accuracy: 0.9946\n",
      "Epoch 12/100\n",
      "26215/26215 [==============================] - 23s 893us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0304 - val_accuracy: 0.9946\n",
      "Epoch 13/100\n",
      "26215/26215 [==============================] - 23s 891us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0305 - val_accuracy: 0.9945\n",
      "Epoch 14/100\n",
      "26215/26215 [==============================] - 23s 896us/step - loss: 0.0302 - accuracy: 0.9946 - val_loss: 0.0306 - val_accuracy: 0.9946\n",
      "Epoch 15/100\n",
      "26215/26215 [==============================] - 23s 890us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0306 - val_accuracy: 0.9946\n",
      "Epoch 16/100\n",
      "26215/26215 [==============================] - 23s 893us/step - loss: 0.0302 - accuracy: 0.9946 - val_loss: 0.0307 - val_accuracy: 0.9946\n",
      "Epoch 17/100\n",
      "26215/26215 [==============================] - 24s 897us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0305 - val_accuracy: 0.9945\n",
      "Epoch 18/100\n",
      "26215/26215 [==============================] - 23s 894us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0305 - val_accuracy: 0.9945\n",
      "Epoch 19/100\n",
      "26215/26215 [==============================] - 23s 890us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0305 - val_accuracy: 0.9946\n",
      "Epoch 20/100\n",
      "26215/26215 [==============================] - 23s 893us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0307 - val_accuracy: 0.9946\n",
      "Epoch 21/100\n",
      "26215/26215 [==============================] - 23s 895us/step - loss: 0.0302 - accuracy: 0.9947 - val_loss: 0.0309 - val_accuracy: 0.9945\n",
      "6554/6554 [==============================] - 4s 633us/step - loss: 0.0304 - accuracy: 0.9946\n",
      "Validation accuracy: 0.9945592880249023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the batch size, the number of epochs, and the input file path\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "input_file = '../lists/preprocessed.csv'\n",
    "\n",
    "# Load the preprocessed features and the class label into a Pandas DataFrame\n",
    "data = pd.read_csv(input_file, header=None, names=['url', 'has_ad', 'is_subdomain', 'num_dots', 'num_hyphens', 'num_digits', 'class'])\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, stratify=data['class'])\n",
    "\n",
    "# Define the input shape of the neural network\n",
    "input_shape = (train_data.shape[1] - 2,)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data.iloc[:, 1:-1], train_data.iloc[:, -1], batch_size=batch_size, epochs=epochs, validation_data=(val_data.iloc[:, 1:-1], val_data.iloc[:, -1]), callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, acc = model.evaluate(val_data.iloc[:, 1:-1], val_data.iloc[:, -1], batch_size=batch_size)\n",
    "print('Validation accuracy:', acc)\n",
    "\n",
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28e2774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "The URL 'www.deheredia.com' is predicted to be not an ad with a probability of 0.99\n"
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "\n",
    "    # Preprocess the input URL to extract the features\n",
    "    url = 'www.deheredia.com'\n",
    "    has_ad = 0\n",
    "    is_subdomain = 0\n",
    "    num_dots = url.count('.')\n",
    "    num_hyphens = url.count('-')\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    input_features = np.array([has_ad, is_subdomain, num_dots, num_hyphens, num_digits]).reshape(1, -1)\n",
    "\n",
    "    # Use the trained model to make a prediction\n",
    "    prediction = model.predict(input_features)[0][0]\n",
    "\n",
    "    # Print the prediction\n",
    "    if prediction > 0.5:\n",
    "        print(f\"The URL '{url}' is predicted to be an ad with a probability of {prediction:.2f}\")\n",
    "    else:\n",
    "        print(f\"The URL '{url}' is predicted to be not an ad with a probability of {1 - prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f6a8705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a URL (e.g. www.google.com): www.deheredia.com\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "The URL 'www.deheredia.com' is predicted to be not an ad with a probability of 0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model from file\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Define a function to extract features from the URL\n",
    "def extract_features(url):\n",
    "    has_ad = 0\n",
    "    is_subdomain = 0\n",
    "    num_dots = url.count('.')\n",
    "    num_hyphens = url.count('-')\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    return np.array([has_ad, is_subdomain, num_dots, num_hyphens, num_digits]).reshape(1, -1)\n",
    "\n",
    "# Define a function to check if a URL is valid\n",
    "def is_valid_url(url):\n",
    "    regex = re.compile(r'(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b')\n",
    "    return regex.match(url) is not None\n",
    "\n",
    "# Get the URL input from the user\n",
    "url = input(\"Enter a URL (e.g. www.google.com): \")\n",
    "\n",
    "# Validate the URL input\n",
    "if not is_valid_url(url):\n",
    "    print(\"Invalid URL.\")\n",
    "else:\n",
    "    # Extract features from the URL\n",
    "    input_features = extract_features(url)\n",
    "\n",
    "    # Use the trained model to make a prediction\n",
    "    prediction = model.predict(input_features)[0][0]\n",
    "\n",
    "    # Print the prediction\n",
    "    if prediction > 0.5:\n",
    "        print(f\"The URL '{url}' is predicted to be an ad with a probability of {prediction:.2f}\")\n",
    "    else:\n",
    "        print(f\"The URL '{url}' is predicted to be not an ad with a probability of {1 - prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9b8481c5c7b37e308517ae4fa47380f470b474e6830fa4718415fe969c43feb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
