{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3543e457",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "- Initially, I had multiple datasets containing lists of only ad servers and non-ad servers. I combined them all to create a dataset 'all.csv'.\n",
    "- Since all.csv had multiple overlapping entries, I deleted the duplicates and saved it as another file 'all-without-duplicates'\n",
    "```\n",
    "df = pd.read_csv(\"../lists/all.csv\",converters={'domain': convert_dtype,'class': convert_dtype}) \n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../lists/all-without-duplicates.csv')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec64c483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazonaws.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>netflix.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474709</th>\n",
       "      <td>slview.psne.jp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474710</th>\n",
       "      <td>x.vipergirls.to</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474711</th>\n",
       "      <td>x0r.urlgalleries.net</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474712</th>\n",
       "      <td>yotta.scrolller.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474713</th>\n",
       "      <td>ytre9jk.txxx.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1474714 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          url class\n",
       "0                  google.com     1\n",
       "1                 youtube.com     1\n",
       "2                facebook.com     1\n",
       "3               amazonaws.com     1\n",
       "4                 netflix.com     1\n",
       "...                       ...   ...\n",
       "1474709        slview.psne.jp     0\n",
       "1474710       x.vipergirls.to     0\n",
       "1474711  x0r.urlgalleries.net     0\n",
       "1474712   yotta.scrolller.com     0\n",
       "1474713      ytre9jk.txxx.com     0\n",
       "\n",
       "[1474714 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "#Convert dtypes for fixing Dtypewarning\n",
    "# https://www.roelpeters.be/solved-dtypewarning-columns-have-mixed-types-specify-dtype-option-on-import-or-set-low-memory-in-pandas/\n",
    "def convert_dtype(x):\n",
    "    if not x:\n",
    "        return ''\n",
    "    try:\n",
    "        return str(x)   \n",
    "    except:        \n",
    "        return ''\n",
    "\n",
    "df = pd.read_csv(\"../lists/all-without-duplicates.csv\",converters={'domain': convert_dtype,'class': convert_dtype}) # Dataset is now stored in a Pandas Dataframe\n",
    "#df = pd.read_csv(\"../lists/all.csv\",converters={'domain': convert_dtype,'class': convert_dtype})\n",
    "#df = df.drop_duplicates()\n",
    "#df.to_csv('../lists/all-without-duplicates.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175ff1d",
   "metadata": {},
   "source": [
    "# Preprocessing and feature extraction\n",
    "This block of code is used for preprocessing the dataset, removing unwanted patterns, and extracting meaningful features from the dataset. Here, the features extracted are has_ad(does it contain the word 'ad'), is_subdomain(does it contain the subdomain 'www'),num_dots(number of dots in the url, excluding subdomain if any),num_hyphens(number of hyphens), num_digits(number of digits in the URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1368e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expressions for pattern matching\n",
    "ad_pattern = r'\\b(ad|ads)\\b'\n",
    "subdomain_pattern = r'^www\\.'\n",
    "dot_pattern = r'.'\n",
    "hyphen_pattern = r'-'\n",
    "digit_pattern = r'\\d'\n",
    "\n",
    "# Define the batch size and the input/output file paths\n",
    "batch_size = 10000\n",
    "input_file = '../lists/all-without-duplicates.csv'\n",
    "output_file = '../lists/preprocessed.csv'\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    # Read the CSV file in chunks\n",
    "    for chunk in pd.read_csv(f_in, chunksize=batch_size):\n",
    "        # Preprocess the URLs in the current chunk\n",
    "        for url in chunk['url']:\n",
    "            has_ad = int(bool(re.search(ad_pattern, url)))\n",
    "            is_subdomain = int(bool(re.search(subdomain_pattern, url)))\n",
    "            num_dots = url.count(dot_pattern) #- is_subdomain\n",
    "            if (is_subdomain == 1):\n",
    "                num_dots = num_dots - 1;\n",
    "            num_hyphens = url.count(hyphen_pattern)\n",
    "            num_digits = len(re.findall(digit_pattern, url))\n",
    "\n",
    "            # Write the preprocessed features to the output file\n",
    "            f_out.write(f'{url},{has_ad},{is_subdomain},{num_dots},{num_hyphens},{num_digits}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a380aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c86075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9b8481c5c7b37e308517ae4fa47380f470b474e6830fa4718415fe969c43feb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
